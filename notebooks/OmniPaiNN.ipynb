{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxh/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from  deepqmc.wf.paulinet.cusp import CuspCorrection, ElectronicAsymptotic\n",
    "from  deepqmc.wf.paulinet.gto import GTOBasis\n",
    "from  deepqmc.wf.paulinet.molorb import MolecularOrbital\n",
    "from  deepqmc.wf.paulinet.omni import OmniSchNet\n",
    "from  deepqmc.wf.paulinet.pyscfext import pyscf_from_mol\n",
    "#from deepqmc.wf import PauliNet\n",
    "\n",
    "\n",
    "\n",
    "from deepqmc import Molecule\n",
    "from deepqmc.physics import pairwise_diffs, pairwise_distance\n",
    "from deepqmc.plugins import PLUGINS\n",
    "from deepqmc.torchext import sloglindet, triu_flat\n",
    "from deepqmc.wf import WaveFunction\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from deepqmc.torchext import SSP, get_log_dnn\n",
    "\n",
    "from deepqmc.wf.paulinet.schnet import ElectronicSchNet, SubnetFactory\n",
    "\n",
    "#__version__ = '0.3.0'\n",
    "#__all__ = ['OmniSchNet']\n",
    "import numpy \n",
    "\n",
    "# %load test.py\n",
    "import torch \n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing, radius_graph\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import ase\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList, Module\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from PaiNN import PaiNNElecNuc\n",
    "from message import MessagePassPaiNN_NE\n",
    "from torch_geometric.nn import radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepqmc import Molecule\n",
    "\n",
    "mol = Molecule.from_name('LiH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jatrow, Backflow, Bipartite, Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0,
     16,
     31,
     52,
     54,
     72,
     100
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class Jastrow(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, embedding_dim, activation_factory=SSP, *, n_layers=3, sum_first=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = get_log_dnn(embedding_dim, 1, activation_factory, n_layers=n_layers)\n",
    "        self.sum_first = sum_first\n",
    "\n",
    "    def forward(self, xs):\n",
    "        if self.sum_first:\n",
    "            xs = self.net(xs.sum(dim=-2))\n",
    "        else:\n",
    "            xs = self.net(xs).sum(dim=-2)\n",
    "        return xs.squeeze(dim=-1)\n",
    "    \n",
    "class Bipartite(Data):\n",
    "    def __init__(self, edge_index, coord_elec, coord_nuc,s_nuc,v_nuc,num_nodes):\n",
    "        super(Bipartite, self).__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.coord_elec = coord_elec\n",
    "        self.coord_nuc = coord_nuc\n",
    "        self.s_nuc = s_nuc\n",
    "        self.v_nuc = v_nuc\n",
    "        self.num_nodes = num_nodes\n",
    "    def __inc__(self, key, value):\n",
    "        if key == 'edge_index':\n",
    "            return torch.tensor([[self.coord_nuc.size(0)], [self.coord_elec.size(0)]])\n",
    "        else:\n",
    "            return super().__inc__(key, value)\n",
    "        \n",
    "class BatchGraphNuc(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        super(BatchGraphNuc, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self,  s_nuc,v_nuc, coord_elec, coord_nuc):\n",
    "        \n",
    "        batch_dim, n_elec = coord_elec.shape[:2]\n",
    "        \n",
    "        \n",
    "        edge_attr =  (coord_elec[..., :, None, :] - coord_nuc[..., None, :, :]).reshape(-1,3)\n",
    "        coord_nuc = coord_nuc.repeat(batch_dim,1,1)\n",
    "        \n",
    "        data_list = [Bipartite(radius(e,n,5.0),e,n,sn,vn, n_elec) \n",
    "                     for e,n,sn,vn in zip(coord_elec, coord_nuc, s_nuc,v_nuc)]\n",
    "        \n",
    "        loader = DataLoader(data_list, batch_size=batch_dim)\n",
    "        batch = next(iter(loader))\n",
    "        \n",
    "        return (batch.s_nuc, batch.v_nuc, batch.edge_index, edge_attr)\n",
    "    \n",
    "class BatchGraphElec(nn.Module):\n",
    "\n",
    "    def __init__(self,cut_off=5.0):\n",
    "        super(BatchGraphElec, self).__init__()\n",
    "        self.cut_off = cut_off\n",
    "\n",
    "    def forward(self,s, v, rs):\n",
    "        # rs are converted to edge_attributes\n",
    "        # num_elec = num_nodes\n",
    "        batch_dim, n_elec = rs.shape[:2] \n",
    "        data = Batch.from_data_list([Data(x=s, v=v, r=r) for s, v, r in zip(s, v, rs)])\n",
    "        \n",
    "        \n",
    "        batch_edge_index = radius_graph(data.r, r=self.cut_off, batch=data.batch, loop=False)\n",
    "\n",
    "        batch_row, batch_col = batch_edge_index\n",
    "        batch_edge_attr = data.r[batch_row] - data.r[batch_col]\n",
    "\n",
    "        return data.x, data.v, batch_edge_index, batch_edge_attr\n",
    "    \n",
    "class Backflow(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        n_orbitals,\n",
    "        n_backflows,\n",
    "        activation_factory=SSP,\n",
    "        *,\n",
    "        n_layers=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        nets = [\n",
    "            get_log_dnn(\n",
    "                embedding_dim,\n",
    "                n_orbitals,\n",
    "                activation_factory,\n",
    "                n_layers=n_layers,\n",
    "                last_bias=True,\n",
    "            )\n",
    "            for _ in range(n_backflows)\n",
    "        ]\n",
    "        self.nets = nn.ModuleList(nets)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return torch.stack([net(xs) for net in self.nets], dim=1) ## Backup Backflow function!!!\n",
    "    \n",
    "class BackflowPaiNN(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        n_backflows,\n",
    "        num_electrons\n",
    "    ):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.net = nn.Sequential(\n",
    "            Linear(embedding_dim, embedding_dim),\n",
    "            Linear(embedding_dim, 1)\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, xs):\n",
    "        return torch.squeeze(self.net(xs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class OmniPaiNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_nuc,\n",
    "        n_up,\n",
    "        n_down,\n",
    "        n_orbitals,\n",
    "        n_backflows,\n",
    "        *,\n",
    "        embedding_dim=128,\n",
    "        num_nodes = 4,\n",
    "        cut_off=5.0,\n",
    "        n_rbf=20,\n",
    "        num_interactions=3,\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.ElectronicPaiNN = PaiNNElecNuc(embedding_dim, \n",
    "                           embedding_dim, \n",
    "                           num_nodes)\n",
    "        \n",
    "        self.jastrow = Jastrow(embedding_dim,sum_first=True)\n",
    "        \n",
    "        self.backflow = BackflowPaiNN(\n",
    "                embedding_dim,\n",
    "                n_backflows,\n",
    "                n_up + n_down\n",
    "                ) \n",
    "        \n",
    "        self.batch_E = BatchGraphElec(cut_off)\n",
    "        self.batch_N = BatchGraphNuc(cut_off)\n",
    "        self.spin_idxs = torch.tensor(\n",
    "            (n_up + n_down) * [0] if n_up == n_down else n_up * [0] + n_down * [1])\n",
    "        \n",
    "        self.nuc_idxs = torch.arange(n_nuc)\n",
    "        self.X = nn.Embedding(1 if n_up == n_down else 2, embedding_dim)\n",
    "        self.Y = nn.Embedding(n_nuc, embedding_dim)\n",
    "        \n",
    "        self.eb = embedding_dim\n",
    "            \n",
    "    def forward(self,rs,rn):\n",
    "        # Took out elect_dists and nuc_dists for try\n",
    "        batch_dim, n_elec = rs.shape[:2]\n",
    "        n_nuc = rn.shape[0]\n",
    "        \n",
    "        # Initializing Scalars and Vectors\n",
    "        s_e = self.X(self.spin_idxs.repeat(batch_dim,1))\n",
    "        s_n = self.Y(self.nuc_idxs.repeat(batch_dim,1))\n",
    "        \n",
    "        v_e = torch.zeros(batch_dim,n_elec,self.eb,3, dtype=torch.float)\n",
    "        v_n = torch.zeros(batch_dim,n_nuc,self.eb,3, dtype=torch.float)\n",
    "\n",
    "        # Creating Batches for e-e-Graph and e-N-Graph\n",
    "        s_e, v_e, edge_index, edge_attr = self.batch_E(s_e, v_e, rs)\n",
    "        s_n, v_n, edge_index_n, edge_attr_n = self.batch_N(s_n, v_n, rs,rn)\n",
    "        \n",
    "        scalars, vectors = self.ElectronicPaiNN(s_e, v_e, s_n,v_n,edge_index, \n",
    "                                        edge_attr, edge_index_n, edge_attr_n)\n",
    "        \n",
    "        \n",
    "        scalars = scalars.reshape(batch_dim, n_elec, -1)\n",
    "        vectors = vectors.reshape(batch_dim, n_elec, self.eb, -1)\n",
    "        \n",
    "        jastrow = self.jastrow(scalars)\n",
    "        backflow = self.backflow(torch.transpose(vectors,-2,-1))\n",
    "        \n",
    "        return jastrow, None, backflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0,
     6,
     12
    ]
   },
   "outputs": [],
   "source": [
    "def eval_slater(xs):\n",
    "    if xs.shape[-1] == 0:\n",
    "        return xs.new_ones(xs.shape[:-2])\n",
    "    return torch.det(xs.contiguous())\n",
    "\n",
    "\n",
    "def eval_log_slater(xs):\n",
    "    if xs.shape[-1] == 0:\n",
    "        return xs.new_ones(xs.shape[:-2]), xs.new_zeros(xs.shape[:-2])\n",
    "    return xs.contiguous().slogdet()\n",
    "\n",
    "\n",
    "class PauliNet(WaveFunction):\n",
    "    \n",
    "\n",
    "    OMNI_FACTORIES = {'omni_schnet': OmniSchNet, 'omni_paiNN': OmniPaiNN}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mol,\n",
    "        basis,\n",
    "        n_configurations=1,\n",
    "        n_orbitals=None,\n",
    "        return_log=True,\n",
    "        use_sloglindet='training',\n",
    "        *,\n",
    "        cusp_correction=True,\n",
    "        cusp_electrons=True,\n",
    "        backflow_type='orbital',\n",
    "        backflow_channels=1,\n",
    "        backflow_transform='mult',\n",
    "        rc_scaling=1.0,\n",
    "        cusp_alpha=10.0,\n",
    "        freeze_embed=False,\n",
    "        omni_factory='omni_paiNN', #omni_schnet\n",
    "        omni_kwargs=None,\n",
    "        cut_off=1.5,\n",
    "        n_rbf=20,\n",
    "        num_interactions=3,\n",
    "        mb_embedding_dim=128\n",
    "    ):\n",
    "        assert use_sloglindet in {'never', 'training', 'always'}\n",
    "        assert return_log or use_sloglindet == 'never'\n",
    "        super().__init__(mol)\n",
    "        n_up, n_down = self.n_up, self.n_down\n",
    "        n_orbitals = n_orbitals or max(n_up, n_down)\n",
    "        confs = [list(range(n_up)) + list(range(n_down))] + [\n",
    "            sum((torch.randperm(n_orbitals)[:n].tolist() for n in (n_up, n_down)), [])\n",
    "            for _ in range(n_configurations - 1)\n",
    "        ]\n",
    "        self.register_buffer('confs', torch.tensor(confs))\n",
    "        self.conf_coeff = (\n",
    "            nn.Linear(n_configurations, 1, bias=False)\n",
    "            if n_configurations > 1\n",
    "            else nn.Identity()\n",
    "        )\n",
    "     \n",
    "        \n",
    "        \n",
    "        self.mo = MolecularOrbital(\n",
    "            mol,\n",
    "            basis,\n",
    "            n_orbitals,\n",
    "            cusp_correction=cusp_correction,\n",
    "            rc_scaling=rc_scaling,\n",
    "        )\n",
    "        self.cusp_same, self.cusp_anti = (\n",
    "            (ElectronicAsymptotic(cusp=cusp, alpha=cusp_alpha) for cusp in (0.25, 0.5))\n",
    "            if cusp_electrons\n",
    "            else (None, None)\n",
    "        )\n",
    "        backflow_spec = {\n",
    "            'orbital': [n_orbitals, backflow_channels],\n",
    "            'det': [max(n_up, n_down), len(self.confs) * backflow_channels],\n",
    "        }[backflow_type]\n",
    "        if backflow_transform == 'both':\n",
    "            backflow_spec[1] *= 2\n",
    "        self.backflow_type = backflow_type\n",
    "        self.backflow_transform = backflow_transform\n",
    "        if 'paulinet.omni_factory' in PLUGINS:\n",
    "            log.info('Using a plugin for paulinet.omni_factory')\n",
    "            omni_factory = PLUGINS['paulinet.omni_factory']\n",
    "        elif isinstance(omni_factory, str):\n",
    "            if omni_kwargs:\n",
    "                omni_kwargs = omni_kwargs[omni_factory]\n",
    "            omni_factory = self.OMNI_FACTORIES[omni_factory]\n",
    "            \n",
    "        #if omni_factory == 'omni_paiNN'\n",
    "        self.omni = (\n",
    "            omni_factory(\n",
    "                len(mol.coords), n_up, n_down, *backflow_spec, **(omni_kwargs or {})\n",
    "            )\n",
    "            if omni_factory\n",
    "            else None\n",
    "        )\n",
    "        \n",
    "        self.return_log = return_log\n",
    "        if freeze_embed:\n",
    "            self.requires_grad_embeddings_(False)\n",
    "        self.n_determinants = len(self.confs) * backflow_channels\n",
    "        if n_up <= 1 or n_down <= 1:\n",
    "            self.use_sloglindet = 'never'\n",
    "            log.warning(\n",
    "                'Setting use_sloglindet to \"never\" as not implemented for n=0 and n=1.'\n",
    "            )\n",
    "        # TODO implement sloglindet for special cases n=0 and n=1\n",
    "        else:\n",
    "            self.use_sloglindet = use_sloglindet\n",
    "            \n",
    "          ###################################################     \n",
    "#         self.omni_painn = OmniPaiNN(\n",
    "#         len(mol.coords),\n",
    "#         n_up,\n",
    "#         n_down,\n",
    "#         n_orbitals,\n",
    "#         *backflow_spec, **(omni_kwargs or {})\n",
    "\n",
    "    def requires_grad_classes_(self, classes, requires_grad):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, classes):\n",
    "                for p in m.parameters(recurse=False):\n",
    "                    p.requires_grad_(requires_grad)\n",
    "        return self\n",
    "\n",
    "    def requires_grad_cusps_(self, requires_grad):\n",
    "        return self.requires_grad_classes_(CuspCorrection, requires_grad)\n",
    "\n",
    "    def requires_grad_embeddings_(self, requires_grad):\n",
    "        return self.requires_grad_classes_(nn.Embedding, requires_grad)\n",
    "\n",
    "    def requires_grad_nets_(self, requires_grad):\n",
    "        return self.requires_grad_classes_(nn.Linear, requires_grad)\n",
    "\n",
    "    @classmethod\n",
    "    def DEFAULTS(cls):\n",
    "        from .omni import Backflow, Jastrow\n",
    "        from .schnet import ElectronicSchNet, SubnetFactory\n",
    "\n",
    "        return {\n",
    "            (cls.from_hf, 'kwargs'): cls.from_pyscf,\n",
    "            (cls.from_pyscf, 'kwargs'): cls,\n",
    "            (cls, 'omni_kwargs'): cls.OMNI_FACTORIES,\n",
    "            (OmniSchNet, 'schnet_kwargs'): ElectronicSchNet,\n",
    "            (OmniSchNet, 'mf_schnet_kwargs'): (ElectronicSchNet, ['version']),\n",
    "            (OmniSchNet, 'subnet_kwargs'): SubnetFactory,\n",
    "            (OmniSchNet, 'mf_subnet_kwargs'): SubnetFactory,\n",
    "            (OmniSchNet, 'jastrow_kwargs'): Jastrow,\n",
    "            (OmniSchNet, 'backflow_kwargs'): Backflow,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_pyscf(\n",
    "        cls,\n",
    "        mf,\n",
    "        *,\n",
    "        init_weights=True,\n",
    "        freeze_mos=True,\n",
    "        freeze_confs=False,\n",
    "        conf_cutoff=1e-2,\n",
    "        conf_limit=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"Construct a :class:`PauliNet` instance from a finished PySCF_ calculation.\n",
    "\n",
    "        Args:\n",
    "            mf (:class:`pyscf.scf.hf.RHF` | :class:`pyscf.mcscf.mc1step.CASSCF`):\n",
    "                restricted (multireference) HF calculation\n",
    "            init_weights (bool): whether molecular orbital coefficients and\n",
    "                configuration coefficients are initialized from the HF calculation\n",
    "            freeze_mos (bool): whether the MO coefficients are frozen for\n",
    "                gradient optimization\n",
    "            freeze_confs (bool): whether the configuration coefficients are\n",
    "                frozen for gradient optimization\n",
    "            conf_cutoff (float): determinants with a linear coefficient above\n",
    "                this threshold are included in the determinant expansion\n",
    "            conf_limit (int): if given, at maximum the given number of configurations\n",
    "                with the largest linear coefficients are used in the ansatz\n",
    "            kwargs: all other arguments are passed to the :class:`PauliNet`\n",
    "                constructor\n",
    "\n",
    "        .. _PySCF: http://pyscf.org\n",
    "        \"\"\"\n",
    "        assert not (set(kwargs) & {'n_configurations', 'n_orbitals'})\n",
    "        n_up, n_down = mf.mol.nelec\n",
    "        if hasattr(mf, 'fcisolver'):\n",
    "            if conf_limit:\n",
    "                conf_cutoff = max(\n",
    "                    np.sort(abs(mf.ci.flatten()))[-conf_limit:][0] - 1e-10, conf_cutoff\n",
    "                )\n",
    "            for tol in [conf_cutoff, conf_cutoff + 2e-10]:\n",
    "                conf_coeff, *confs = zip(\n",
    "                    *mf.fcisolver.large_ci(\n",
    "                        mf.ci, mf.ncas, mf.nelecas, tol=tol, return_strs=False\n",
    "                    )\n",
    "                )\n",
    "                if not conf_limit or len(conf_coeff) <= conf_limit:\n",
    "                    break\n",
    "            else:\n",
    "                raise AssertionError()\n",
    "            # discard the last ci wave function if degenerate\n",
    "            ns_dbl = n_up - mf.nelecas[0], n_down - mf.nelecas[1]\n",
    "            conf_coeff = torch.tensor(conf_coeff)\n",
    "            confs = [\n",
    "                [\n",
    "                    torch.arange(n_dbl, dtype=torch.long).expand(len(conf_coeff), -1),\n",
    "                    torch.tensor(cfs, dtype=torch.long) + n_dbl,\n",
    "                ]\n",
    "                for n_dbl, cfs in zip(ns_dbl, confs)\n",
    "            ]\n",
    "            confs = [torch.cat(cfs, dim=-1) for cfs in confs]\n",
    "            confs = torch.cat(confs, dim=-1)\n",
    "            kwargs['n_configurations'] = len(confs)\n",
    "            kwargs['n_orbitals'] = confs.max().item() + 1\n",
    "        else:\n",
    "            confs = None\n",
    "        mol = Molecule(\n",
    "            mf.mol.atom_coords().astype('float32'),\n",
    "            mf.mol.atom_charges(),\n",
    "            mf.mol.charge,\n",
    "            mf.mol.spin,\n",
    "        )\n",
    "        basis = GTOBasis.from_pyscf(mf.mol)\n",
    "        wf = cls(mol, basis, **kwargs)\n",
    "        if init_weights:\n",
    "            wf.mo.init_from_pyscf(mf, freeze_mos=freeze_mos)\n",
    "            if confs is not None:\n",
    "                wf.confs.detach().copy_(confs)\n",
    "                if len(confs) > 1:\n",
    "                    wf.conf_coeff.weight.detach().copy_(conf_coeff)\n",
    "                if freeze_confs:\n",
    "                    wf.conf_coeff.weight.requires_grad_(False)\n",
    "        return wf\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_hf(cls, mol, *, basis='6-311g', cas=None, workdir=None, **kwargs):\n",
    "        r\"\"\"Construct a :class:`PauliNet` instance by running a HF calculation.\n",
    "\n",
    "        This is the top-level interface.\n",
    "\n",
    "        Args:\n",
    "            mol (:class:`~deepqmc.Molecule`): molecule whose wave function\n",
    "                is represented\n",
    "            basis (str): basis of the internal HF calculation\n",
    "            cas ((int, int)): tuple of the number of active orbitals and number of\n",
    "                active electrons for a complete active space multireference\n",
    "                HF calculation\n",
    "            workdir (str): path where PySCF calculations are cached\n",
    "            kwargs: all other arguments are passed to :func:`PauliNet.from_pyscf`\n",
    "        \"\"\"\n",
    "        mf, mc = pyscf_from_mol(mol, basis, cas, workdir)\n",
    "        assert bool(cas) == bool(mc)\n",
    "        wf = PauliNet.from_pyscf(mc or mf, **kwargs)\n",
    "        wf.mf = mf\n",
    "        return wf\n",
    "\n",
    "\n",
    "    def pop_charges(self):\n",
    "        try:\n",
    "            mf = self.mf\n",
    "        except AttributeError:\n",
    "            return super().pop_charges()\n",
    "        return self.mol.charges.new(mf.pop(verbose=0)[1])\n",
    "\n",
    "    def _backflow_op(self, xs, fs):\n",
    "        if self.backflow_transform == 'mult':\n",
    "            fs_mult, fs_add = fs, None\n",
    "            print(fs_mult.shape)\n",
    "        elif self.backflow_transform == 'add':\n",
    "            fs_mult, fs_add = None, fs\n",
    "        elif self.backflow_transform == 'both':\n",
    "            fs_mult, fs_add = fs[:, : fs.shape[1] // 2], fs[:, fs.shape[1] // 2 :]\n",
    "        if fs_add is not None:\n",
    "            envel = (xs ** 2).mean(dim=-1, keepdim=True).sqrt()\n",
    "        if fs_mult is not None:\n",
    "            xs = xs * (1 + 2 * torch.tanh(fs_mult / 4))\n",
    "        if fs_add is not None:\n",
    "            xs = xs + 0.1 * envel * torch.tanh(fs_add / 4)\n",
    "        return xs\n",
    "#################################################################################################\n",
    "    def forward(self, rs):  # noqa: C901\n",
    "        batch_dim, n_elec = rs.shape[:2]\n",
    "        \n",
    "        \n",
    "        assert n_elec == self.confs.shape[1]\n",
    "        n_atoms = len(self.mol)\n",
    "        coords = self.mol.coords\n",
    "        \n",
    "        J, fs, ps = self.omni(rs, coords) # ps: particle shift\n",
    "        rs = rs + ps\n",
    "        \n",
    "        diffs_nuc = pairwise_diffs(torch.cat([coords, rs.flatten(end_dim=1)]), coords)\n",
    "        dists_elec = pairwise_distance(rs, rs)\n",
    "        if self.omni:\n",
    "            dists_nuc = (\n",
    "                diffs_nuc[n_atoms:, :, 3].sqrt().view(batch_dim, n_elec, n_atoms)\n",
    "            )\n",
    "            \n",
    "                 \n",
    "        xs = self.mo(diffs_nuc)\n",
    "        # get orbitals as [bs, 1, i, mu]\n",
    "        xs = xs.view(batch_dim, 1, n_elec, -1)\n",
    "        # get jastrow J and backflow fs (as [bs, q, i, mu/nu])\n",
    "        #J, fs = self.omni(dists_nuc, dists_elec) if self.omni else (None, None)\n",
    "        if fs is not None and self.backflow_type == 'orbital':\n",
    "            xs = self._backflow_op(xs, fs)\n",
    "        # form dets as [bs, q, p, i, nu]\n",
    "        conf_up, conf_down = self.confs[:, : self.n_up], self.confs[:, self.n_up :]\n",
    "        det_up = xs[:, :, : self.n_up, conf_up].transpose(-3, -2)\n",
    "        det_down = xs[:, :, self.n_up :, conf_down].transpose(-3, -2)\n",
    "        if fs is not None and self.backflow_type == 'det':\n",
    "            n_conf = len(self.confs)\n",
    "            fs = fs.unflatten(1, ((None, fs.shape[1] // n_conf), (None, n_conf)))\n",
    "            det_up = self._backflow_op(det_up, fs[..., : self.n_up, : self.n_up])\n",
    "            det_down = self._backflow_op(det_down, fs[..., self.n_up :, : self.n_down])\n",
    "            # with open-shell systems, part of the backflow output is not used\n",
    "        if self.use_sloglindet == 'always' or (\n",
    "            self.use_sloglindet == 'training' and not self.sampling\n",
    "        ):\n",
    "            bf_dim = det_up.shape[-4]\n",
    "            if isinstance(self.conf_coeff, nn.Linear):\n",
    "                conf_coeff = self.conf_coeff.weight[0]\n",
    "                conf_coeff = conf_coeff.expand(bf_dim, -1).flatten() / np.sqrt(bf_dim)\n",
    "            else:\n",
    "                conf_coeff = det_up.new_ones(1)\n",
    "            det_up = det_up.flatten(start_dim=-4, end_dim=-3).contiguous()\n",
    "            det_down = det_down.flatten(start_dim=-4, end_dim=-3).contiguous()\n",
    "            sign, psi = sloglindet(conf_coeff, det_up, det_down)\n",
    "            sign = sign.detach()\n",
    "        else:\n",
    "            if self.return_log:\n",
    "                sign_up, det_up = eval_log_slater(det_up)\n",
    "                sign_down, det_down = eval_log_slater(det_down)\n",
    "                xs = det_up + det_down\n",
    "                xs_shift = xs.flatten(start_dim=1).max(dim=-1).values\n",
    "                # the exp-normalize trick, to avoid over/underflow of the exponential\n",
    "                xs = sign_up * sign_down * torch.exp(xs - xs_shift[:, None, None])\n",
    "            else:\n",
    "                det_up = eval_slater(det_up)\n",
    "                det_down = eval_slater(det_down)\n",
    "                xs = det_up * det_down\n",
    "            psi = self.conf_coeff(xs).squeeze(dim=-1).mean(dim=-1)\n",
    "            if self.return_log:\n",
    "                psi, sign = psi.abs().log() + xs_shift, psi.sign().detach()\n",
    "        if self.cusp_same:\n",
    "            cusp_same = self.cusp_same(\n",
    "                torch.cat(\n",
    "                    [triu_flat(dists_elec[:, idxs, idxs]) for idxs in self.spin_slices],\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "            cusp_anti = self.cusp_anti(\n",
    "                dists_elec[:, : self.n_up, self.n_up :].flatten(start_dim=1)\n",
    "            )\n",
    "            psi = (\n",
    "                psi + cusp_same + cusp_anti\n",
    "                if self.return_log\n",
    "                else psi * torch.exp(cusp_same + cusp_anti)\n",
    "            )\n",
    "        if J is not None:\n",
    "            psi = psi + J if self.return_log else psi * torch.exp(J)\n",
    "        return (psi, sign) if self.return_log else psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def pairwise_distance(coords1, coords2):\n",
    "#     return (coords1[..., :, None, :] - coords2[..., None, :, :]).norm(dim=-1)\n",
    "\n",
    "# def pairwise_diffs(coords1, coords2, axes_offset=True):\n",
    "#     diffs = coords1[..., :, None, :] - coords2[..., None, :, :]\n",
    "#     if axes_offset:\n",
    "#         diffs = offset_from_axes(diffs)\n",
    "#     return torch.cat([diffs, (diffs ** 2).sum(dim=-1, keepdim=True)], dim=-1)\n",
    "# def offset_from_axes(rs):\n",
    "#     eps = rs.new_tensor(100 * torch.finfo(rs.dtype).eps)\n",
    "#     offset = torch.where(rs < 0, -eps, eps)\n",
    "#     return torch.where(rs.abs() < eps, rs + offset, rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = Molecule(  # LiH\n",
    "    coords=[[0.0, 0.0, 0.0], [3.0, 0.0, 0.0]],\n",
    "    charges=[3, 1],\n",
    "    charge=0,\n",
    "    spin=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -7.98461406083101\n",
      "CASSCF energy = -7.98461406083102\n",
      "CASCI E = -7.98461406083102  E(CI) = -8.98461406083102  S^2 = 0.0000000\n"
     ]
    }
   ],
   "source": [
    "net = PauliNet.from_hf(mol, cas=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepqmc import train\n",
    "#train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_up = 2\n",
    "n_down = 2\n",
    "batch_dim = 10\n",
    "num_elec = n_up + n_down\n",
    "num_nuc = 2\n",
    "rn = torch.rand(num_nuc,3)\n",
    "rs = torch.rand(batch_dim,num_elec,3)\n",
    "#Paramerts\n",
    "# F: Num. features, r_ij: cartesian positions\n",
    "F = int(128)\n",
    "\n",
    "# Embedding Functions for Scalars\n",
    "spin_idxs = torch.tensor(\n",
    "            (n_up + n_down) * [0] if n_up == n_down else n_up * [0] + n_down * [1]).repeat(batch_dim,1)\n",
    "\n",
    "nuc_idxs = torch.arange(num_nuc).repeat(batch_dim,1)\n",
    "\n",
    "# s0 = X(spin_idxs)\n",
    "# sn = Y(nuc_idxs)\n",
    "\n",
    "# Features Scalars and Vectors\n",
    "#s_old = torch.rand(batch_dim,num_elec,F, dtype=torch.float)\n",
    "v0 = torch.zeros(batch_dim,num_elec,F,3, dtype=torch.float)\n",
    "#sn = torch.rand(batch_dim,num_nuc,F, dtype=torch.float)\n",
    "vn = torch.zeros(batch_dim,num_nuc,F,3, dtype=torch.float)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = OmniPaiNN(num_nuc,n_up, n_down ,1,1,4,1.5,num_elec,3,128)\n",
    "jast, fs, ps = test(rs,rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
